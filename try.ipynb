{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from transformers import BertModel, BertTokenizer\n",
    "\n",
    "# Torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Parameters\n",
    "\n",
    "# Preprocessing Parameters\n",
    "validation_size = 0.2\n",
    "RAND_STATE = 5780\n",
    "shuffle_split = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def load_npz(file_path):\n",
    "    with np.load(file_path) as data:\n",
    "        return {key: data[key] for key in data}\n",
    "\n",
    "train_data = load_npz(r'.\\data\\train.npz')\n",
    "test_data = load_npz(r'.\\data\\test.npz')\n",
    "train_emb1, train_emb2, train_labels = train_data['emb1'], train_data['emb2'], train_data['preference']\n",
    "test_emb1, test_emb2 = test_data['emb1'], test_data['emb2']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'uid': array([    0,     1,     2, ..., 18747, 18748, 18749], dtype=int64),\n",
       " 'emb1': array([[-0.05075016, -0.03491386, -0.05787281, ...,  0.00020284,\n",
       "          0.02388327, -0.02491781],\n",
       "        [-0.12402835, -0.07631648, -0.05782915, ...,  0.02713838,\n",
       "          0.01394665,  0.0186507 ],\n",
       "        [-0.06794146, -0.0385992 ,  0.04476113, ...,  0.07999779,\n",
       "          0.04943484,  0.00783883],\n",
       "        ...,\n",
       "        [ 0.02096516, -0.00752076, -0.06958353, ...,  0.01346127,\n",
       "          0.01917063, -0.06059628],\n",
       "        [-0.00901941,  0.01330765, -0.02343761, ..., -0.02690429,\n",
       "          0.0084649 ,  0.01999134],\n",
       "        [-0.05510234,  0.00251053, -0.01775946, ...,  0.00322949,\n",
       "         -0.02700103,  0.01986161]], dtype=float32),\n",
       " 'emb2': array([[-0.03255587,  0.01327268, -0.00508326, ..., -0.01196616,\n",
       "         -0.03564733, -0.03713938],\n",
       "        [-0.00014027,  0.03904634,  0.0592997 , ...,  0.00117963,\n",
       "          0.04012304,  0.07394706],\n",
       "        [-0.068197  , -0.0943828 ,  0.04236921, ...,  0.0225933 ,\n",
       "          0.00185285, -0.03076085],\n",
       "        ...,\n",
       "        [ 0.00845952,  0.00125914, -0.03183057, ..., -0.04645595,\n",
       "         -0.00618974,  0.00794393],\n",
       "        [-0.05969298,  0.00475971,  0.00906092, ..., -0.0083008 ,\n",
       "         -0.05037842, -0.02749569],\n",
       "        [-0.04472147, -0.01137812, -0.05518954, ..., -0.05703627,\n",
       "          0.03633969,  0.00122035]], dtype=float32),\n",
       " 'preference': array([1, 1, 1, ..., 1, 0, 0], dtype=int8)}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of 'uid': 18750\n",
      "Length of 'emb1': 18750\n",
      "Length of 'emb2': 18750\n",
      "Length of 'preference': 18750\n"
     ]
    }
   ],
   "source": [
    "for key, value in train_data.items():\n",
    "    print(f\"Length of '{key}': {len(value) if isinstance(value, np.ndarray) else 'Not an array'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['uid', 'emb1', 'emb2', 'preference'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(384,)\n",
      "(384,)\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# x1\n",
    "print(train_data['emb1'][0].shape) # (384,)\n",
    "# x2\n",
    "print(train_data['emb2'][0].shape) # (384,)\n",
    "# y\n",
    "print(train_data['preference'][0]) # 1\n",
    "# train_data['emb1'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_validation_split(Xs, Ys, validation_size: float=0.2):\n",
    "    Xs_tr, Xs_va, Ys_tr, Ys_va = train_test_split(Xs, Ys, test_size=validation_size, random_state=RAND_STATE, shuffle=shuffle_split, stratify=Ys)\n",
    "    return torch.Tensor(Xs_tr), torch.Tensor(Xs_va), torch.Tensor(Ys_tr).long(), torch.Tensor(Ys_va).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18750, 384)\n",
      "(18750, 384)\n",
      "Xs_tr.shape: torch.Size([15000, 768])\n",
      "Ys_tr.shape: torch.Size([15000])\n",
      "Xs_va.shape: torch.Size([3750, 768])\n",
      "Ys_va.shape: torch.Size([3750])\n"
     ]
    }
   ],
   "source": [
    "print(train_data['emb1'].shape) # (n x d): (18750, 384)\n",
    "print(train_data['emb2'].shape) # (n x d): (18750, 384)\n",
    "\n",
    "# Concatenate the input in to a single long vector\n",
    "Xs = np.concatenate((train_data['emb1'], train_data['emb2']), axis=1)\n",
    "Ys = train_data['preference']\n",
    "\n",
    "# Train Validation Split\n",
    "Xs_tr, Xs_va, Ys_tr, Ys_va = train_validation_split(Xs, Ys, validation_size)\n",
    "\n",
    "# Convert to Torch\n",
    "print(f'Xs_tr.shape: {Xs_tr.shape}') # (15000, 768)\n",
    "print(f'Ys_tr.shape: {Ys_tr.shape}') # (15000,)\n",
    "print(f'Xs_va.shape: {Xs_va.shape}') # (3750, 768)\n",
    "print(f'Ys_va.shape: {Ys_va.shape}') # (3750,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0525,  0.0358, -0.1479,  ..., -0.0207, -0.0516, -0.0119],\n",
       "        [-0.0813, -0.0833,  0.0329,  ...,  0.0286,  0.0443,  0.0085],\n",
       "        [-0.0362, -0.0262,  0.0381,  ...,  0.0300,  0.0761,  0.0385],\n",
       "        ...,\n",
       "        [ 0.0056, -0.0284, -0.0130,  ...,  0.0343,  0.0795,  0.0295],\n",
       "        [-0.0359, -0.0749, -0.0522,  ..., -0.0366, -0.0189, -0.0320],\n",
       "        [-0.0801, -0.0949,  0.0201,  ...,  0.0674, -0.0343,  0.0003]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xs_tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 1,  ..., 1, 0, 0])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ys_tr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "embedding_dim = 768\n",
    "hidden_dim = 128\n",
    "output_dim = 2\n",
    "num_layers = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FFNN Model\n",
    "class FFNN(nn.Module):\n",
    "    def __init__(self, embedding_dim: int, hidden_dim: int, output_dim: int, num_layers: int = 1) -> None:\n",
    "        super().__init__()\n",
    "        assert num_layers > 0\n",
    "\n",
    "        self.num_layers = num_layers\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layers.append(nn.Linear(embedding_dim, hidden_dim))\n",
    "        for _ in range(num_layers - 1):\n",
    "            self.layers.append(nn.Linear(hidden_dim, hidden_dim))\n",
    "        self.output_layer = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, embeddings: torch.Tensor) -> torch.Tensor:\n",
    "        x = embeddings\n",
    "        for layer in self.layers:\n",
    "            x = F.relu(layer(x))\n",
    "        output = self.output_layer(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFNN(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, output_dim, num_layers=1):\n",
    "        super().__init__()\n",
    "        assert num_layers > 0\n",
    "\n",
    "        self.layers = nn.ModuleList([nn.Linear(embedding_dim, hidden_dim)])\n",
    "        self.batch_norms = nn.ModuleList([nn.BatchNorm1d(hidden_dim)])\n",
    "\n",
    "        for _ in range(num_layers - 1):\n",
    "            self.layers.append(nn.Linear(hidden_dim, hidden_dim))\n",
    "            self.batch_norms.append(nn.BatchNorm1d(hidden_dim))\n",
    "\n",
    "        self.output_layer = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, embeddings):\n",
    "        x = embeddings\n",
    "        for layer, batch_norm in zip(self.layers, self.batch_norms):\n",
    "            x = F.relu(layer(x))\n",
    "            x = batch_norm(x)\n",
    "        output = self.output_layer(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FFNN(\n",
       "  (layers): ModuleList(\n",
       "    (0): Linear(in_features=768, out_features=128, bias=True)\n",
       "    (1-2): 2 x Linear(in_features=128, out_features=128, bias=True)\n",
       "  )\n",
       "  (batch_norms): ModuleList(\n",
       "    (0-2): 3 x BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (output_layer): Linear(in_features=128, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test\n",
    "ffnn = FFNN(embedding_dim=embedding_dim, hidden_dim=hidden_dim, output_dim=output_dim, num_layers=num_layers)\n",
    "ffnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, output_dim, num_layers):\n",
    "        super().__init__()\n",
    "        self.hidden_layers = nn.RNN(embedding_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.output_layer = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass through the RNN layer\n",
    "        out, _ = self.hidden_layers(x)\n",
    "\n",
    "        # Take the output from the last time step and pass it through the fully connected layer\n",
    "        out = self.output_layer(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNN(\n",
       "  (hidden_layers): RNN(768, 128, num_layers=3, batch_first=True)\n",
       "  (output_layer): Linear(in_features=128, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn = RNN(embedding_dim, hidden_dim, output_dim, num_layers)\n",
    "rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, output_dim, num_layers):\n",
    "        super().__init__()\n",
    "        self.hidden_layers = nn.LSTM(embedding_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.output_layer = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass through the RNN layer\n",
    "        out, _ = self.hidden_layers(x)\n",
    "\n",
    "        # Take the output from the last time step and pass it through the fully connected layer\n",
    "        out = self.output_layer(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTM(\n",
       "  (hidden_layers): LSTM(768, 128, num_layers=3, batch_first=True)\n",
       "  (output_layer): Linear(in_features=128, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm = LSTM(embedding_dim, hidden_dim, output_dim, num_layers)\n",
    "lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT(nn.Module):\n",
    "    def __init__(self, embedding_dim, output_dim=2):\n",
    "        super(BERT, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.classifier = nn.Linear(embedding_dim, output_dim)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.pooler_output\n",
    "        return self.classifier(pooled_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT(nn.Module):\n",
    "    def __init__(self, embedding_dim, output_dim=2):\n",
    "        super(BERT, self).__init__()\n",
    "        self.reduction_layer = nn.Linear(embedding_dim, 512)\n",
    "        self.classifier = nn.Linear(512, output_dim)\n",
    "\n",
    "    # def forward(self, x):\n",
    "    #     reduced_x = self.reduction_layer(x)\n",
    "    #     # You can add activation functions and other layers here if needed\n",
    "    #     output = self.classifier(reduced_x)\n",
    "    #     return output\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.pooler_output\n",
    "        return self.classifier(pooled_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class BERT(nn.Module):\n",
    "    def __init__(self, output_dim=2, dropout_rate=0.1):\n",
    "        super(BERT, self).__init__()\n",
    "        # Initialize the BERT model\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        \n",
    "        # Dropout layer for regularization\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "        # Classifier layer\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, output_dim)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        # BERT model outputs\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        # Pooler output for classification\n",
    "        pooled_output = outputs.pooler_output\n",
    "\n",
    "        # Apply dropout\n",
    "        dropped_out = self.dropout(pooled_output)\n",
    "\n",
    "        # Pass through the classifier\n",
    "        output = self.classifier(dropped_out)\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BERT(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# bert = BERT(embedding_dim, hidden_dim, output_dim, num_layers)\n",
    "bert = BERT(output_dim)\n",
    "bert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "epochs = 10\n",
    "batch_size = 128\n",
    "lr = 0.00001\n",
    "rho1 = 0.99\n",
    "rho2 = 0.999\n",
    "grad_clip_max_norm = 1\n",
    "sgd_optimizer = torch.optim.SGD(ffnn.parameters(), lr=lr)\n",
    "adam_optimizer = torch.optim.Adam(ffnn.parameters(), lr=lr)\n",
    "binary_cross_entropy_loss_fn = torch.nn.BCELoss()\n",
    "cross_entropy_loss_fn = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate a trained model on MNIST data\n",
    "#\n",
    "# dataloader    dataloader of examples to evaluate on\n",
    "# model         trained PyTorch model\n",
    "# loss_fn       loss function (e.g. torch.nn.CrossEntropyLoss)\n",
    "#\n",
    "# returns       tuple of (loss, accuracy), both python floats\n",
    "@torch.no_grad()\n",
    "def evaluate_model(Xs_va, Ys_va, model, loss_fn):\n",
    "\tmodel.eval()\n",
    "\ttotal_loss = 0.0\n",
    "\ttotal_correct = 0\n",
    "\ttotal_samples = 0\n",
    "\n",
    "\t# Create DataLoader for batching\n",
    "\tvalidation_dataset = TensorDataset(Xs_va, Ys_va)\n",
    "\tvalidation_loader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=True)\t\n",
    "\n",
    "\tfor X, Y in validation_loader:\n",
    "\t\tY_pred_prob = model(X)\n",
    "\t\tloss = loss_fn(Y_pred_prob, Y)\n",
    "\t\ttotal_loss += loss.item()\n",
    "\t\n",
    "\t\tY_pred = torch.argmax(Y_pred_prob, dim=1)\n",
    "\t\ttotal_correct += torch.sum(Y_pred == Y).item()\n",
    "\t\ttotal_samples += Y.size(0)\n",
    "\t\n",
    "\taverage_loss = total_loss / len(validation_loader)\n",
    "\taccuracy = total_correct / total_samples\n",
    "\t\n",
    "\treturn average_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(Xs_tr, Ys_tr, Xs_va, Ys_va, model, loss_fn, optimizer, epochs, grad_clip_max_norm):\n",
    "\t# Create DataLoader for batching\n",
    "\ttrain_dataset = TensorDataset(Xs_tr, Ys_tr)\n",
    "\ttrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\t\n",
    "\tfor epoch in range(epochs):\n",
    "\t\t# Set to training mode\n",
    "\t\tmodel.train()\n",
    "\t\t\n",
    "\t\tfor i, (X, Y) in enumerate(train_loader):\n",
    "\t\t\ttotal_loss = 0.0\n",
    "\n",
    "\t\t\t# Zero gradients for every batch\n",
    "\t\t\toptimizer.zero_grad()\n",
    "\n",
    "\t\t\t# Make predictions for this batch\n",
    "\t\t\tY_pred_prob = model(X)\n",
    "\n",
    "\t\t\t# Compute the loss and its gradients\n",
    "\t\t\tloss = loss_fn(Y_pred_prob, Y)\n",
    "\t\t\tloss.backward()\n",
    "\n",
    "\t\t\tif grad_clip_max_norm is not None:\n",
    "\t\t\t\tnn.utils.clip_grad_norm_(model.parameters(), max_norm=grad_clip_max_norm)\n",
    "\n",
    "\t\t\t# Adjust learning weights\n",
    "\t\t\toptimizer.step()\n",
    "\n",
    "\t\t\t# Gather data and report\n",
    "\t\t\ttotal_loss += loss.item()\n",
    "\t\t\n",
    "\t\t# Evaluate the model\n",
    "\t\tvalidation_loss, validation_accuracy = evaluate_model(Xs_va, Ys_va, model, loss_fn)\n",
    "\t\tprint(f\"Epoch {epoch+1}/{epochs}, Training Loss: {loss.item()}, Validation Loss: {round(validation_loss,3)}, Validation Accuracy: {round(validation_accuracy,3)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Training Loss: 0.019268997013568878, Validation Loss: 0.664, Validation Accuracy: 0.871\n",
      "Epoch 2/10, Training Loss: 0.006506010424345732, Validation Loss: 0.644, Validation Accuracy: 0.871\n",
      "Epoch 3/10, Training Loss: 0.010621930472552776, Validation Loss: 0.669, Validation Accuracy: 0.873\n",
      "Epoch 4/10, Training Loss: 0.0071776434779167175, Validation Loss: 0.663, Validation Accuracy: 0.875\n",
      "Epoch 5/10, Training Loss: 0.0022742196451872587, Validation Loss: 0.681, Validation Accuracy: 0.87\n",
      "Epoch 6/10, Training Loss: 0.000254071899689734, Validation Loss: 0.639, Validation Accuracy: 0.876\n",
      "Epoch 7/10, Training Loss: 0.04095885530114174, Validation Loss: 0.664, Validation Accuracy: 0.874\n",
      "Epoch 8/10, Training Loss: 0.004265537019819021, Validation Loss: 0.668, Validation Accuracy: 0.873\n",
      "Epoch 9/10, Training Loss: 0.00638449564576149, Validation Loss: 0.648, Validation Accuracy: 0.876\n",
      "Epoch 10/10, Training Loss: 0.01604512147605419, Validation Loss: 0.691, Validation Accuracy: 0.873\n"
     ]
    }
   ],
   "source": [
    "# FFNN\n",
    "train(Xs_tr, Ys_tr, Xs_va, Ys_va, ffnn, cross_entropy_loss_fn, adam_optimizer, epochs, grad_clip_max_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Training Loss: 0.7021800875663757, Validation Loss: 0.694, Validation Accuracy: 0.501\n",
      "Epoch 2/10, Training Loss: 0.6896276473999023, Validation Loss: 0.694, Validation Accuracy: 0.496\n",
      "Epoch 3/10, Training Loss: 0.6949682235717773, Validation Loss: 0.694, Validation Accuracy: 0.495\n",
      "Epoch 4/10, Training Loss: 0.6878487467765808, Validation Loss: 0.694, Validation Accuracy: 0.5\n",
      "Epoch 5/10, Training Loss: 0.6873521208763123, Validation Loss: 0.694, Validation Accuracy: 0.494\n",
      "Epoch 6/10, Training Loss: 0.6904792785644531, Validation Loss: 0.694, Validation Accuracy: 0.505\n",
      "Epoch 7/10, Training Loss: 0.6857607364654541, Validation Loss: 0.694, Validation Accuracy: 0.498\n",
      "Epoch 8/10, Training Loss: 0.6856743693351746, Validation Loss: 0.694, Validation Accuracy: 0.498\n",
      "Epoch 9/10, Training Loss: 0.6999891400337219, Validation Loss: 0.694, Validation Accuracy: 0.495\n",
      "Epoch 10/10, Training Loss: 0.6845292448997498, Validation Loss: 0.694, Validation Accuracy: 0.502\n"
     ]
    }
   ],
   "source": [
    "# RNN\n",
    "train(Xs_tr, Ys_tr, Xs_va, Ys_va, rnn, cross_entropy_loss_fn, adam_optimizer, epochs, grad_clip_max_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Training Loss: 0.6923296451568604, Validation Loss: 0.693, Validation Accuracy: 0.5\n",
      "Epoch 2/10, Training Loss: 0.6969357132911682, Validation Loss: 0.693, Validation Accuracy: 0.5\n",
      "Epoch 3/10, Training Loss: 0.685727059841156, Validation Loss: 0.693, Validation Accuracy: 0.5\n",
      "Epoch 4/10, Training Loss: 0.6915701031684875, Validation Loss: 0.693, Validation Accuracy: 0.5\n",
      "Epoch 5/10, Training Loss: 0.6898727416992188, Validation Loss: 0.693, Validation Accuracy: 0.5\n",
      "Epoch 6/10, Training Loss: 0.6936323642730713, Validation Loss: 0.693, Validation Accuracy: 0.5\n",
      "Epoch 7/10, Training Loss: 0.6918451189994812, Validation Loss: 0.693, Validation Accuracy: 0.5\n",
      "Epoch 8/10, Training Loss: 0.6927154660224915, Validation Loss: 0.693, Validation Accuracy: 0.5\n",
      "Epoch 9/10, Training Loss: 0.6960940361022949, Validation Loss: 0.693, Validation Accuracy: 0.5\n",
      "Epoch 10/10, Training Loss: 0.6906998753547668, Validation Loss: 0.693, Validation Accuracy: 0.5\n"
     ]
    }
   ],
   "source": [
    "# LSTM\n",
    "train(Xs_tr, Ys_tr, Xs_va, Ys_va, lstm, cross_entropy_loss_fn, adam_optimizer, epochs, grad_clip_max_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # BERT\n",
    "# train(Xs_tr, Ys_tr, Xs_va, Ys_va, bert, cross_entropy_loss_fn, adam_optimizer, epochs, grad_clip_max_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # time too long to run\n",
    "# from sklearn.metrics import classification_report, accuracy_score\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.svm import SVC\n",
    "\n",
    "# scaler = StandardScaler()\n",
    "\n",
    "# # Fit only on training data\n",
    "# Xs_tr_scaled = scaler.fit_transform(Xs_tr)\n",
    "# Xs_va_scaled = scaler.transform(Xs_va)\n",
    "\n",
    "# # Initialize the SVM classifier\n",
    "# clf = SVC(kernel='linear')\n",
    "\n",
    "# # Train the classifier\n",
    "# clf.fit(Xs_tr_scaled, Ys_tr)\n",
    "\n",
    "# # Predict on the validation set\n",
    "# Ys_va_pred = clf.predict(Xs_va_scaled)\n",
    "\n",
    "# # Evaluate the model\n",
    "# print(\"Accuracy:\", accuracy_score(Ys_va, Ys_va_pred))\n",
    "# print(classification_report(Ys_va, Ys_va_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Prediction & Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prediction(Xs_te, model):\n",
    "    Y_preds_prob = model(Xs_te)\n",
    "    Y_preds = torch.argmax(Y_preds_prob, axis = 1)\n",
    "    return Y_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_submission(uid, Y_preds):\n",
    "    df = pd.DataFrame({'uid': uid, 'preference': Y_preds})\n",
    "    df.to_csv('submission.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xs_te = np.concatenate((test_data['emb1'], test_data['emb2']), axis=1)\n",
    "Xs_te = torch.Tensor(Xs_te)\n",
    "Y_preds = make_prediction(Xs_te, ffnn)\n",
    "make_submission(test_data['uid'], np.array(Y_preds))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
